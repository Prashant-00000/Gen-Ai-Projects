# -*- coding: utf-8 -*-
"""Advanced RAG Implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_8dmYm1km1CYqQhfkP4Jo-2-AQQvzIno
"""

!pip install -U langchain langchain-community langchain-text-splitters faiss-cpu sentence-transformers rank-bm25 transformers accelerate

with open("knowledge.txt", "w") as f:
    f.write("""
Artificial Intelligence enables machines to mimic human intelligence.
Machine learning is a subset of AI.
Deep learning uses neural networks.
Hybrid search combines dense and sparse retrieval.
Reranking improves search result accuracy.
Vector databases store embeddings.
""")

from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

loader = TextLoader("knowledge.txt")
documents = loader.load()

splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=50
)

docs = splitter.split_documents(documents)

print("Chunks:", len(docs))

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

vectorstore = FAISS.from_documents(docs, embeddings)

from rank_bm25 import BM25Okapi

corpus = [doc.page_content.split() for doc in docs]
bm25 = BM25Okapi(corpus)

pip install rank-bm25

"""HYBRID SEARCH( It combines both sematic and Bm25)"""

import numpy as np

def hybrid_search(query, top_k=5):

    # Dense search
    dense_docs = vectorstore.similarity_search(query, k=top_k)

    # Sparse search
    tokenized_query = query.split()
    sparse_scores = bm25.get_scores(tokenized_query)
    top_sparse_indices = np.argsort(sparse_scores)[-top_k:]
    sparse_docs = [docs[i] for i in top_sparse_indices]

    # Merge results (remove duplicates)
    merged = {doc.page_content: doc for doc in dense_docs + sparse_docs}

    return list(merged.values())

from sentence_transformers import CrossEncoder

reranker = CrossEncoder(
    "cross-encoder/ms-marco-MiniLM-L-6-v2",
)

def rerank(query, candidates, top_k=3):

    pairs = [(query, doc.page_content) for doc in candidates]
    scores = reranker.predict(pairs)

    ranked = sorted(
        zip(candidates, scores),
        key=lambda x: x[1],
        reverse=True
    )

    return [doc for doc, score in ranked[:top_k]]

from transformers import pipeline
from langchain_community.llms import HuggingFacePipeline

hf_pipeline = pipeline(
    "text-generation",
    model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    max_new_tokens=150,
    temperature=0.0,
    do_sample=False,
    device_map="auto"
)

llm = HuggingFacePipeline(pipeline=hf_pipeline)

def advanced_rag(query):

    # Hybrid retrieval
    candidates = hybrid_search(query, top_k=5)

    # Rerank
    top_docs = rerank(query, candidates, top_k=3)

    # Build context
    context = "\n\n".join([doc.page_content for doc in top_docs])

    prompt = f"""
Use the context below to answer the question accurately.

Context:
{context}

Question:
{query}

Answer:
"""

    return llm.invoke(prompt)

print(advanced_rag("What is hybrid search?"))

